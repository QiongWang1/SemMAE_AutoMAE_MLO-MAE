# Evaluation of Baseline Methods from the MLO-MAE Paper on PathMNIST

## 1. Introduction
- Goal: test whether the DermaMNIST advantages of[MLO-MAE](https://arxiv.org/abs/2402.18128) transfer to the larger [PathMNIST](https://medmnist.com/)  colorectal histopathology dataset.
- Setup: reproduce ViT, MAE, U-MAE, SemMAE, AutoMAE, and MLO-MAE using the unified SCC training + evaluation pipeline.
- Motivation: assess how multi-level optimization and learnable masking generalize to complex pathology imagery.

## 2. Experimental Setup
- Dataset: PathMNIST, 9-class colorectal cancer histopathology classification.
- Input: 3Ã—32Ã—32 (resized from 28Ã—28).
- Splits: trainâ€¯89â€¯996, validationâ€¯10â€¯004, testâ€¯7â€¯180.
- Metrics: accuracy, precision, recall, macro-averaged F1.

## 3. Results and Comparative Evaluation
### 3.1 Comparative Performance Table
| Metric / Model | ViT | MAE | U-MAE | SemMAE | AutoMAE | MLO-MAE |
| --- | --- | --- | --- | --- | --- | --- |
| Masking Strategy | No pretraining | Random masking | Random masking | Learnable masking | Learnable masking | Learnable + multi-level optimization |
| Accuracy (%) | 80.99 | 84.29 | 82.58 | 83.23 | 85.54 | 89.04 ðŸ¥‡ |
| Precision (%) | 77.08 | 80.18 | 78.49 | 84.14 | 86.59 | 89.19 |
| Recall (%) | 76.72 | 81.26 | 78.63 | 83.23 | 85.54 | 89.04 |
| F1-Score (%) | 75.59 | 80.37 | 77.92 | 83.44 | 85.64 | 88.94 |

*Note: precision, recall, F1 are macro-averaged to ensure class-level fairness.*

### 3.2 Performance Ranking by Accuracy
1. MLO-MAE â€” 89.04â€¯% (+3.5â€¯% vs. AutoMAE) ðŸ¥‡  
2. AutoMAE â€” 85.54â€¯%  
3. MAE â€” 84.29â€¯%  
4. SemMAE â€” 83.23â€¯%  
5. U-MAE â€” 82.58â€¯%  
6. ViT â€” 80.99â€¯%

### 3.3 Balanced Performance of MLO-MAE
- Achieves the best metrics across accuracy (89.04â€¯%), precision (89.19â€¯%), recall (89.04â€¯%), and F1 (88.94â€¯%).
- Indicates multi-level optimization improves accuracy while keeping macro performance balanced.

## 4. Overall Analysis
### 4.1 Performance Trend
- Random masking (MAE/U-MAE) â†’ +3â€“4â€¯% over ViT.
- Learnable masking (SemMAE/AutoMAE) â†’ additional +1â€“2â€¯%.
- Multi-level learnable masking (MLO-MAE) â†’ +3.5â€¯% vs. AutoMAE, +8â€¯% vs. ViT.
- Trend mirrors DermaMNIST, showing dataset-agnostic gains.

### 4.2 Dataset Adaptation
- PathMNIST involves high-resolution patches with complex textures and color variation.
- MLO-MAEâ€™s multi-level region selection captures nuclear morphology and tissue organization, driving accuracy gains.

### 4.3 Observations
- AutoMAE/SemMAE plateau near 84â€“85â€¯%.
- MLO-MAE keeps improving, yielding +3.3â€¯% F1 over AutoMAE.
- Larger dataset and intra-class variability highlight the impact of multi-scale mask optimization.

## 5. Detailed Experimental Insights
1. **Random Masking vs. No Pretraining**
- MAE and U-MAE (~83â€“84â€¯%) beat ViT (80.99â€¯%), evidencing masked autoencodingâ€™s robustness boost.

2. **Learnable Masking Advantages**
- SemMAE and AutoMAE gain â‰ˆ1â€“2â€¯% over random masking through attention-guided mask selection.

3. **Multi-Level Optimization Effectiveness**
- MLO-MAEâ€™s 89.04â€¯% accuracy (+3.5â€¯% vs. AutoMAE) shows hierarchical mask optimization enriches both texture and semantic representations.

4. **Medical Imaging Challenges**
- Pathology images exhibit strong appearance variability and fine-grained cues.
- Multi-scale refinement bridges patch-level context with global tissue structure for better diagnostics.


## 6. Key Findings
- Learnable masking beats random masking; MLO-MAE tops accuracy at 89.04â€¯%.
- Multi-level optimization delivers +3.5â€¯% over best single-level learnable masking (AutoMAE 85.54â€¯%).
- Texture-rich medical datasets benefit most from hierarchical mask selection.
- All masked autoencoders outperform ViT, reaffirming self-supervised pretraining for biomedical imaging.

## 7. Conclusion
- PathMNIST evaluation confirms DermaMNIST findings generalize to pathology data.
- MLO-MAE achieves 89.04â€¯% accuracy and 88.94â€¯% F1, surpassing all baselines.
- Consistent gains across skin lesions and colorectal histopathology evidence multi-level optimizationâ€™s robustness and scalability.

## 8. References
1. **ViT**
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un- terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 

2. **AME**
- Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr DollÃ¡r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 16000â€“16009, 2022. 

3. **U-MAE**
- Qi Zhang, Yifei Wang, and Yisen Wang. How mask matters: Towards theoretical understandings of masked autoencoders. Advances in Neural Information Processing Systems, 35:27127â€“27139, 2022. 

4. **SemMAE** 
- Gang Li, Heliang Zheng, Daqing Liu, Chaoyue Wang, Bing Su, and Changwen Zheng. Semmae: Semantic- guided masking for learning masked autoencoders. Advances in Neural Information Processing Systems, 35:14290â€“14302, 2022. 

5. **AutoMAE**
- Haijian Chen, Wendong Zhang, Yunbo Wang, and Xiaokang Yang. Improving masked autoencoders by learning where to mask. arXiv preprint arXiv:2303.06583, 2023. 

6. **MLO-MAE**
- Han Guo, Ramtin Hosseini, Ruiyi Zhang, Sai Ashish Somayajula, Ranak Roy Chowdhury, Rajesh K. Gupta, Pengtao Xie. Downstream Task Guided Masking Learning in Masked Autoencoders Using Multi-Level Optimization. https://arxiv.org/abs/2402.18128